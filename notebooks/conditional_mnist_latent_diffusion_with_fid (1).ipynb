{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb3ab0b",
      "metadata": {
        "id": "4cb3ab0b"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision matplotlib tqdm\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d16ed06",
      "metadata": {
        "id": "9d16ed06"
      },
      "outputs": [],
      "source": [
        "# --- Data: MNIST in [-1,1] ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x * 2.0 - 1.0)  # [0,1] -> [-1,1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "imgs, labels = next(iter(train_loader))\n",
        "print(\"Batch shape:\", imgs.shape, \"Labels shape:\", labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab19a21a",
      "metadata": {
        "id": "ab19a21a"
      },
      "outputs": [],
      "source": [
        "# --- Convolutional VAE ---\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, latent_channels=4):\n",
        "        super().__init__()\n",
        "        self.latent_channels = latent_channels\n",
        "        self.H_lat = 7\n",
        "        self.W_lat = 7\n",
        "        self.latent_dim = latent_channels * self.H_lat * self.W_lat\n",
        "\n",
        "        # Encoder: 1x28x28 -> 32x14x14 -> 64x7x7\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 14x14\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # 7x7\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.enc_fc_mu = nn.Linear(64 * 7 * 7, self.latent_dim)\n",
        "        self.enc_fc_logvar = nn.Linear(64 * 7 * 7, self.latent_dim)\n",
        "\n",
        "        # Decoder: latent_dim -> 64x7x7 -> 32x14x14 -> 1x28x28\n",
        "        self.dec_fc = nn.Linear(self.latent_dim, 64 * 7 * 7)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # 14x14\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),   # 28x28\n",
        "            nn.Tanh(),  # output in [-1,1]\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.enc(x)\n",
        "        h = h.view(x.size(0), -1)\n",
        "        mu = self.enc_fc_mu(h)\n",
        "        logvar = self.enc_fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z_vec):\n",
        "        h = self.dec_fc(z_vec)\n",
        "        h = h.view(z_vec.size(0), 64, 7, 7)\n",
        "        x_recon = self.dec(h)\n",
        "        return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z_vec = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z_vec)\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "    def encode_to_2d_latent(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z_vec = self.reparameterize(mu, logvar)\n",
        "        z_2d = z_vec.view(x.size(0), self.latent_channels, self.H_lat, self.W_lat)\n",
        "        return z_2d\n",
        "\n",
        "    def decode_from_2d_latent(self, z_2d):\n",
        "        z_vec = z_2d.view(z_2d.size(0), -1)\n",
        "        return self.decode(z_vec)\n",
        "\n",
        "def vae_loss(x_recon, x, mu, logvar):\n",
        "    recon_loss = F.mse_loss(x_recon, x, reduction=\"sum\") / x.size(0)\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
        "    return recon_loss + kl, recon_loss, kl\n",
        "\n",
        "vae = ConvVAE(latent_channels=4).to(device)\n",
        "vae_opt = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"VAE parameters (M):\", sum(p.numel() for p in vae.parameters())/1e6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e152505",
      "metadata": {
        "id": "3e152505"
      },
      "outputs": [],
      "source": [
        "# --- Train VAE ---\n",
        "num_vae_epochs = 50  # increase for better reconstructions\n",
        "\n",
        "vae.train()\n",
        "for epoch in range(num_vae_epochs):\n",
        "    pbar = tqdm(train_loader, desc=f\"VAE Epoch {epoch+1}/{num_vae_epochs}\")\n",
        "    total_loss = 0.0\n",
        "    for imgs, _ in pbar:\n",
        "        imgs = imgs.to(device)\n",
        "        x_recon, mu, logvar = vae(imgs)\n",
        "        loss, r_loss, kl = vae_loss(x_recon, imgs, mu, logvar)\n",
        "\n",
        "        vae_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        vae_opt.step()\n",
        "\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        pbar.set_postfix({\"loss\": loss.item(), \"recon\": r_loss.item(), \"kl\": kl.item()})\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: avg loss = {total_loss/len(train_dataset):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70bf23a",
      "metadata": {
        "id": "c70bf23a"
      },
      "outputs": [],
      "source": [
        "# --- Quick VAE reconstruction check ---\n",
        "vae.eval()\n",
        "\n",
        "imgs, _ = next(iter(train_loader))\n",
        "imgs = imgs.to(device)[:8]\n",
        "with torch.no_grad():\n",
        "    recon, _, _ = vae(imgs)\n",
        "\n",
        "def show_batch(x, title):\n",
        "    x = (x.cpu() + 1) / 2  # [-1,1] -> [0,1]\n",
        "    grid = torch.cat([xx for xx in x], dim=2)[0]\n",
        "    plt.figure(figsize=(8,2))\n",
        "    plt.imshow(grid, cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "show_batch(imgs, \"Original\")\n",
        "show_batch(recon, \"VAE Reconstruction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c532c07",
      "metadata": {
        "id": "2c532c07"
      },
      "outputs": [],
      "source": [
        "# --- Diffusion setup in latent space ---\n",
        "T = 200\n",
        "beta_start = 1e-4\n",
        "beta_end   = 0.02\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, T).to(device)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), alphas_cumprod[:-1]], dim=0)\n",
        "\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "\n",
        "def q_sample(z0, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(z0)\n",
        "    sqrt_ac = sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "    sqrt_om = sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "    return sqrt_ac * z0 + sqrt_om * noise\n",
        "\n",
        "def sample_timesteps(batch_size):\n",
        "    return torch.randint(low=0, high=T, size=(batch_size,), device=device)\n",
        "\n",
        "def sinusoidal_time_embedding(timesteps, dim):\n",
        "    device_ = timesteps.device\n",
        "    half_dim = dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, device=device_) * -emb)\n",
        "    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if dim % 2 == 1:\n",
        "        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
        "    return emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5e4943",
      "metadata": {
        "id": "7b5e4943"
      },
      "outputs": [],
      "source": [
        "# --- Conditional U-Net in latent space ---\n",
        "time_embed_dim = 64\n",
        "text_embed_dim = 64\n",
        "base_channels  = 64\n",
        "num_classes    = 10\n",
        "guidance_scale = 3.0\n",
        "drop_cond_prob = 0.1\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, cond_dim):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(8, in_ch)\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        self.cond_proj = nn.Linear(cond_dim, 2 * out_ch)\n",
        "\n",
        "        if in_ch != out_ch:\n",
        "            self.skip = nn.Conv2d(in_ch, out_ch, 1)\n",
        "        else:\n",
        "            self.skip = nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        gamma_beta = self.cond_proj(cond)\n",
        "        gamma, beta = gamma_beta.chunk(2, dim=1)\n",
        "        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n",
        "        beta = beta.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        h = self.norm1(x)\n",
        "        h = F.silu(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = h * (1 + gamma) + beta\n",
        "        h = F.silu(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        return h + self.skip(x)\n",
        "\n",
        "class LatentUNet(nn.Module):\n",
        "    def __init__(self, in_ch=4, base_ch=64, time_dim=64, text_dim=64):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "        )\n",
        "        self.text_mlp = nn.Sequential(\n",
        "            nn.Linear(text_dim, text_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(text_dim, text_dim),\n",
        "        )\n",
        "        cond_dim = time_dim + text_dim\n",
        "\n",
        "        self.conv_in = nn.Conv2d(in_ch, base_ch, 3, padding=1)\n",
        "        self.down1 = ResidualBlock(base_ch, base_ch, cond_dim)\n",
        "        self.down2 = ResidualBlock(base_ch, base_ch * 2, cond_dim)\n",
        "        self.pool1 = nn.AvgPool2d(2)\n",
        "        self.down3 = ResidualBlock(base_ch * 2, base_ch * 2, cond_dim)\n",
        "\n",
        "        self.mid = ResidualBlock(base_ch * 2, base_ch * 2, cond_dim)\n",
        "\n",
        "        self.up1 = ResidualBlock(base_ch * 2, base_ch * 2, cond_dim)\n",
        "        self.up2 = ResidualBlock(base_ch * 2, base_ch, cond_dim)\n",
        "        self.conv_out = nn.Conv2d(base_ch, in_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, t, text_emb):\n",
        "        t_emb = sinusoidal_time_embedding(t, time_embed_dim)\n",
        "        t_emb = self.time_mlp(t_emb)\n",
        "        text_emb = self.text_mlp(text_emb)\n",
        "        cond = torch.cat([t_emb, text_emb], dim=1)\n",
        "\n",
        "        x = self.conv_in(x)\n",
        "        x1 = self.down1(x, cond)\n",
        "        x2 = self.down2(x1, cond)\n",
        "        x2p = self.pool1(x2)\n",
        "        x3 = self.down3(x2p, cond)\n",
        "\n",
        "        m = self.mid(x3, cond)\n",
        "\n",
        "        u1 = self.up1(m, cond)\n",
        "        u1_up = F.interpolate(u1, size=x2.shape[-2:], mode=\"nearest\")\n",
        "        u2 = self.up2(u1_up, cond)\n",
        "        out = self.conv_out(u2)\n",
        "        return out\n",
        "\n",
        "class LabelTextEncoder(nn.Module):\n",
        "    def __init__(self, num_classes=10, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_classes, embed_dim)\n",
        "\n",
        "    def forward(self, labels):\n",
        "        return self.emb(labels)\n",
        "\n",
        "latent_channels = 4\n",
        "vae.eval()\n",
        "for p in vae.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "unet = LatentUNet(in_ch=latent_channels,\n",
        "                  base_ch=base_channels,\n",
        "                  time_dim=time_embed_dim,\n",
        "                  text_dim=text_embed_dim).to(device)\n",
        "text_encoder = LabelTextEncoder(num_classes, text_embed_dim).to(device)\n",
        "\n",
        "diff_opt = torch.optim.AdamW(\n",
        "    list(unet.parameters()) + list(text_encoder.parameters()),\n",
        "    lr=2e-4\n",
        ")\n",
        "\n",
        "print(\"Diffusion params (M):\",\n",
        "      (sum(p.numel() for p in unet.parameters()) +\n",
        "       sum(p.numel() for p in text_encoder.parameters()))/1e6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25763735",
      "metadata": {
        "id": "25763735"
      },
      "outputs": [],
      "source": [
        "# --- Train latent diffusion (conditional on labels) ---\n",
        "num_diff_epochs = 20  # increase for better sample quality\n",
        "\n",
        "unet.train()\n",
        "text_encoder.train()\n",
        "\n",
        "for epoch in range(num_diff_epochs):\n",
        "    pbar = tqdm(train_loader, desc=f\"Diff Epoch {epoch+1}/{num_diff_epochs}\")\n",
        "    for imgs, labels in pbar:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z0 = vae.encode_to_2d_latent(imgs)\n",
        "\n",
        "        t = sample_timesteps(z0.size(0))\n",
        "        noise = torch.randn_like(z0)\n",
        "        z_t = q_sample(z0, t, noise)\n",
        "\n",
        "        text_emb = text_encoder(labels)\n",
        "        drop_mask = (torch.rand(z0.size(0), device=device) < drop_cond_prob).float().view(-1, 1)\n",
        "        text_emb = text_emb * (1.0 - drop_mask)\n",
        "\n",
        "        noise_pred = unet(z_t, t, text_emb)\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        diff_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        diff_opt.step()\n",
        "\n",
        "        pbar.set_postfix({\"loss\": loss.item()})\n",
        "    print(f\"Epoch {epoch+1}: last batch loss = {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a52942b",
      "metadata": {
        "id": "6a52942b"
      },
      "outputs": [],
      "source": [
        "# --- Sampling: label -> latent diffusion -> VAE decoder ---\n",
        "@torch.no_grad()\n",
        "def p_sample(x_t, t, text_emb, text_emb_uncond, guidance_scale=3.0):\n",
        "    eps_cond = unet(x_t, t, text_emb)\n",
        "    eps_uncond = unet(x_t, t, text_emb_uncond)\n",
        "    eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
        "\n",
        "    beta_t = betas[t].view(-1,1,1,1)\n",
        "    sqrt_om = sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n",
        "    sqrt_recip_a = sqrt_recip_alphas[t].view(-1,1,1,1)\n",
        "\n",
        "    ac_t = alphas_cumprod[t].view(-1,1,1,1)\n",
        "    x0_pred = (x_t - sqrt_om * eps) / torch.sqrt(ac_t)\n",
        "    mean = sqrt_recip_a * (x_t - beta_t / sqrt_om * eps)\n",
        "\n",
        "    if t[0] > 0:\n",
        "        noise = torch.randn_like(x_t)\n",
        "        var = posterior_variance[t].view(-1,1,1,1)\n",
        "        x_prev = mean + torch.sqrt(var) * noise\n",
        "    else:\n",
        "        x_prev = mean\n",
        "    return x_prev\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_from_labels(labels, num_steps=100, guidance_scale=3.0):\n",
        "    vae.eval()\n",
        "    unet.eval()\n",
        "    text_encoder.eval()\n",
        "\n",
        "    if not torch.is_tensor(labels):\n",
        "        labels = torch.tensor(labels, device=device, dtype=torch.long)\n",
        "    else:\n",
        "        labels = labels.to(device)\n",
        "    B = labels.size(0)\n",
        "\n",
        "    z_t = torch.randn(B, latent_channels, 7, 7, device=device)\n",
        "\n",
        "    text_emb = text_encoder(labels)\n",
        "    text_emb_uncond = torch.zeros_like(text_emb)\n",
        "\n",
        "    for step in tqdm(reversed(range(num_steps)), total=num_steps, desc=\"Sampling\"):\n",
        "        t = torch.full((B,), step, device=device, dtype=torch.long)\n",
        "        z_t = p_sample(z_t, t, text_emb, text_emb_uncond, guidance_scale)\n",
        "\n",
        "    imgs = vae.decode_from_2d_latent(z_t)\n",
        "    imgs = (imgs.clamp(-1,1) + 1) / 2.0\n",
        "    return imgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceef9d6c",
      "metadata": {
        "id": "ceef9d6c"
      },
      "outputs": [],
      "source": [
        "# --- Visualize samples for digits 0..9 ---\n",
        "labels_to_generate = [0,1,2,3,4,5,6,7,8,9]\n",
        "samples = sample_from_labels(labels_to_generate, num_steps=100, guidance_scale=3.0)\n",
        "samples = samples.cpu()\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "for i, lbl in enumerate(labels_to_generate):\n",
        "    plt.subplot(2, (len(labels_to_generate)+1)//2, i+1)\n",
        "    plt.imshow(samples[i,0], cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"digit {lbl}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d29430b",
      "metadata": {
        "id": "9d29430b"
      },
      "outputs": [],
      "source": [
        "# --- FID Evaluation Utilities ---\n",
        "import numpy as np\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_inception_model(device):\n",
        "    inception = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
        "    inception.fc = nn.Identity()\n",
        "    inception.eval().to(device)\n",
        "    return inception\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_inception_features(x, model, device):\n",
        "    if x.size(1) == 1:\n",
        "        x = x.repeat(1,3,1,1)\n",
        "    x = F.interpolate(x, size=(299,299), mode=\"bilinear\", align_corners=False)\n",
        "    x = x.to(device)\n",
        "    feat = model(x)\n",
        "    if isinstance(feat, tuple):\n",
        "        feat = feat[0]\n",
        "    return feat.cpu().numpy()\n",
        "\n",
        "def compute_activation_statistics_from_loader(loader, model, num_samples=10000, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    feats = []\n",
        "    seen = 0\n",
        "    for imgs, _ in tqdm(loader, desc=\"Real features for FID\"):\n",
        "        imgs = (imgs + 1) / 2.0\n",
        "        bs = imgs.size(0)\n",
        "        if seen + bs > num_samples:\n",
        "            imgs = imgs[:num_samples-seen]\n",
        "            bs = imgs.size(0)\n",
        "        f = get_inception_features(imgs, model, device)\n",
        "        feats.append(f)\n",
        "        seen += bs\n",
        "        if seen >= num_samples:\n",
        "            break\n",
        "    feats = np.concatenate(feats, axis=0)\n",
        "    mu = np.mean(feats, axis=0)\n",
        "    sigma = np.cov(feats, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "def compute_activation_statistics_from_generator(gen_fn, model, num_samples=10000, batch_size=64, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    feats = []\n",
        "    seen = 0\n",
        "    while seen < num_samples:\n",
        "        cur_bs = min(batch_size, num_samples-seen)\n",
        "        imgs = gen_fn(cur_bs)\n",
        "        f = get_inception_features(imgs, model, device)\n",
        "        feats.append(f)\n",
        "        seen += imgs.size(0)\n",
        "    feats = np.concatenate(feats, axis=0)\n",
        "    mu = np.mean(feats, axis=0)\n",
        "    sigma = np.cov(feats, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "def sqrtm_psd(mat):\n",
        "    vals, vecs = np.linalg.eigh(mat)\n",
        "    vals = np.maximum(vals, 0)\n",
        "    return (vecs * np.sqrt(vals)).dot(vecs.T)\n",
        "\n",
        "def calculate_fid(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    diff = mu1 - mu2\n",
        "    covmean = sqrtm_psd(sigma1.dot(sigma2) + eps * np.eye(sigma1.shape[0]))\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return float(fid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cdd8659",
      "metadata": {
        "id": "3cdd8659"
      },
      "outputs": [],
      "source": [
        "# --- Compute FID between real MNIST and generated samples ---\n",
        "inception = get_inception_model(device)\n",
        "\n",
        "# 1) Real stats\n",
        "mu_real, cov_real = compute_activation_statistics_from_loader(\n",
        "    train_loader,\n",
        "    inception,\n",
        "    num_samples=500,  # can increase for better estimate\n",
        "    device=device\n",
        ")\n",
        "print(\"Real stats done.\")\n",
        "\n",
        "# 2) Fake stats from diffusion model\n",
        "@torch.no_grad()\n",
        "def diffusion_generate_fn(batch_size):\n",
        "    labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "    imgs = sample_from_labels(labels, num_steps=100, guidance_scale=3.0)\n",
        "    return imgs\n",
        "\n",
        "mu_fake, cov_fake = compute_activation_statistics_from_generator(\n",
        "    diffusion_generate_fn,\n",
        "    inception,\n",
        "    num_samples=500,\n",
        "    batch_size=64,\n",
        "    device=device\n",
        ")\n",
        "print(\"Fake stats done.\")\n",
        "\n",
        "fid_value = calculate_fid(mu_real, cov_real, mu_fake, cov_fake)\n",
        "print(f\"FID (latent diffusion vs real MNIST): {fid_value:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}