{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3ae3c65",
      "metadata": {
        "id": "c3ae3c65"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82611cbe",
      "metadata": {
        "id": "82611cbe"
      },
      "outputs": [],
      "source": [
        "# If you are on Colab, uncomment this to ensure all dependencies are there.\n",
        "# In most recent Colab environments, torch/torchvision are already installed.\n",
        "# !pip install torch torchvision matplotlib tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf6694b",
      "metadata": {
        "id": "ddf6694b"
      },
      "source": [
        "## 2. Device and Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898db882",
      "metadata": {
        "id": "898db882"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    import random, numpy as np\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1c7880",
      "metadata": {
        "id": "ae1c7880"
      },
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99a9024",
      "metadata": {
        "id": "c99a9024"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 128\n",
        "num_epochs = 50        # For a quick classroom demo\n",
        "lr = 2e-4\n",
        "\n",
        "# Diffusion hyperparameters\n",
        "T = 250                # Number of diffusion steps (smaller for speed)\n",
        "beta_start = 1e-4\n",
        "beta_end = 0.02\n",
        "\n",
        "# Data / image settings\n",
        "image_size = 28\n",
        "num_channels = 1       # MNIST is grayscale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce78ac36",
      "metadata": {
        "id": "ce78ac36"
      },
      "source": [
        "## 4. MNIST Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f416b312",
      "metadata": {
        "id": "f416b312"
      },
      "outputs": [],
      "source": [
        "# Normalize images to [-1, 1] (common for diffusion models)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),              # [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # -> [-1, 1] for 1 channel\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "len(train_dataset), len(train_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c628f3",
      "metadata": {
        "id": "21c628f3"
      },
      "source": [
        "## 5. Diffusion Schedule & Forward (Noising) Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a64442",
      "metadata": {
        "id": "68a64442"
      },
      "outputs": [],
      "source": [
        "# Linear beta schedule\n",
        "betas = torch.linspace(beta_start, beta_end, T).to(device)   # (T,)\n",
        "alphas = 1.0 - betas                                         # (T,)\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)                # alpha_bar_t\n",
        "alphas_cumprod_prev = torch.cat(\n",
        "    [torch.tensor([1.0], device=device), alphas_cumprod[:-1]],\n",
        "    dim=0,\n",
        ")\n",
        "\n",
        "# Precomputed terms\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb19ab39",
      "metadata": {
        "id": "bb19ab39"
      },
      "outputs": [],
      "source": [
        "def get_index_from_list(vals, t, x_shape):\n",
        "    # Helper to pick values for a batch of indices t and reshape to x_shape.\n",
        "    # vals: (T,) tensor, t: (B,) tensor of timesteps.\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t)              # (B,)\n",
        "    return out.view(batch_size, *((1,) * (len(x_shape) - 1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39050d6d",
      "metadata": {
        "id": "39050d6d"
      },
      "outputs": [],
      "source": [
        "def q_sample(x_start, t, noise=None):\n",
        "    # Diffuse the data: q(x_t | x_0).\n",
        "    # x_start: (B, C, H, W), t: (B,) timesteps.\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    # q(x_t | x_0) = N( sqrt(alpha_bar_t) * x_0, (1 - alpha_bar_t) I )\n",
        "    return sqrt_alphas_cumprod_t * x_start + \\\n",
        "           sqrt_one_minus_alphas_cumprod_t * noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f9d2d45",
      "metadata": {
        "id": "6f9d2d45"
      },
      "source": [
        "### 5.1 Visualizing the Forward (Noising) Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7daf7d",
      "metadata": {
        "id": "6c7daf7d"
      },
      "outputs": [],
      "source": [
        "def show_image_grid(images, title='', nrow=8):\n",
        "    # Denormalize from [-1,1] back to [0,1]\n",
        "    images = (images + 1) * 0.5\n",
        "    images = torch.clamp(images, 0.0, 1.0)\n",
        "    grid = utils.make_grid(images, nrow=nrow)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    # For single-channel images, repeat channels for visualization\n",
        "    grid_np = grid.cpu().numpy()\n",
        "    if grid_np.shape[0] == 1:\n",
        "        grid_np = grid_np.repeat(3, axis=0)\n",
        "    plt.imshow(grid_np.transpose(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "# Take a small batch and show different timesteps\n",
        "examples = next(iter(train_loader))[0][:8].to(device)  # (8,1,28,28)\n",
        "\n",
        "with torch.no_grad():\n",
        "    t_steps = torch.tensor(\n",
        "        [0, T//10, T//5, T//2, T-1], device=device\n",
        "    )\n",
        "    imgs = []\n",
        "    for t_scalar in t_steps:\n",
        "        t = torch.full(\n",
        "            (examples.shape[0],),\n",
        "            t_scalar,\n",
        "            device=device,\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        noisy = q_sample(examples, t)\n",
        "        imgs.append(noisy)\n",
        "\n",
        "imgs_to_show = torch.cat(imgs, dim=0)\n",
        "show_image_grid(\n",
        "    imgs_to_show,\n",
        "    title='Forward noising at different timesteps (groups from t=0 to tâ‰ˆT)',\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e360cc5",
      "metadata": {
        "id": "4e360cc5"
      },
      "source": [
        "## 6. U-Net Noise Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9179d602",
      "metadata": {
        "id": "9179d602"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPosEmb(nn.Module):\n",
        "    # Sinusoidal timestep embedding (like in the original DDPM).\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t: (B,) int64 timesteps -> (B, dim) embedding\n",
        "        device = t.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb_factor = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
        "        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "        return emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59984ce3",
      "metadata": {
        "id": "59984ce3"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "        self.act = nn.SiLU()\n",
        "        self.res_conv = (\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
        "            if in_ch != out_ch else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        # x: (B, C, H, W), t_emb: (B, time_emb_dim)\n",
        "        h = self.conv1(x)\n",
        "        time_emb = self.time_mlp(t_emb)           # (B, out_ch)\n",
        "        h = h + time_emb[..., None, None]         # broadcast\n",
        "        h = self.act(h)\n",
        "        h = self.conv2(h)\n",
        "        return self.act(h + self.res_conv(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28624c49",
      "metadata": {
        "id": "28624c49"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=1,\n",
        "                 base_channels=32,\n",
        "                 channel_mults=(1, 2, 4),\n",
        "                 time_emb_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding MLP\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPosEmb(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim * 4, time_emb_dim),\n",
        "        )\n",
        "\n",
        "        # Downsampling path\n",
        "        dims = [base_channels * m for m in channel_mults]\n",
        "        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.downs_pools = nn.ModuleList()\n",
        "        in_ch = base_channels\n",
        "        for dim in dims:\n",
        "            self.downs.append(ResidualBlock(in_ch, dim, time_emb_dim))\n",
        "            self.downs_pools.append(nn.MaxPool2d(2))\n",
        "            in_ch = dim\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid_block1 = ResidualBlock(in_ch, in_ch, time_emb_dim)\n",
        "        self.mid_block2 = ResidualBlock(in_ch, in_ch, time_emb_dim)\n",
        "\n",
        "        # Upsampling path\n",
        "        self.ups_transpose = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        rev_dims = list(reversed(dims))\n",
        "        for dim in rev_dims:\n",
        "            self.ups_transpose.append(\n",
        "                nn.ConvTranspose2d(in_ch, dim, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.ups.append(\n",
        "                ResidualBlock(dim * 2, dim, time_emb_dim)  # concat skip\n",
        "            )\n",
        "            in_ch = dim\n",
        "\n",
        "        self.conv_out = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # t: (B,)\n",
        "        t_emb = self.time_mlp(t)  # (B, time_emb_dim)\n",
        "\n",
        "        # Down\n",
        "        x = self.conv_in(x)\n",
        "        skips = []\n",
        "        for down, pool in zip(self.downs, self.downs_pools):\n",
        "            x = down(x, t_emb)\n",
        "            skips.append(x)\n",
        "            x = pool(x)\n",
        "\n",
        "        # Middle\n",
        "        x = self.mid_block1(x, t_emb)\n",
        "        x = self.mid_block2(x, t_emb)\n",
        "\n",
        "        # Up\n",
        "        for up_transpose, up, skip in zip(\n",
        "            self.ups_transpose, self.ups, reversed(skips)\n",
        "        ):\n",
        "            x = up_transpose(x)\n",
        "            # Handle any size mismatch (padding)\n",
        "            if x.shape[-1] != skip.shape[-1]:\n",
        "                diff = skip.shape[-1] - x.shape[-1]\n",
        "                x = F.pad(x, (0, diff, 0, diff))\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = up(x, t_emb)\n",
        "\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "model = UNet(in_channels=num_channels).to(device)\n",
        "print('Model parameters (M):', sum(p.numel() for p in model.parameters()) / 1e6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c16fd744",
      "metadata": {
        "id": "c16fd744"
      },
      "source": [
        "## 7. Training Objective (Noise Prediction Loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc0de4f",
      "metadata": {
        "id": "efc0de4f"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "def p_losses(model, x_start, t, noise=None):\n",
        "    # Simplified DDPM loss: MSE between true noise and predicted noise.\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = model(x_noisy, t)\n",
        "    loss = F.mse_loss(predicted_noise, noise)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374888d2",
      "metadata": {
        "id": "374888d2"
      },
      "source": [
        "## 8. Sampling (Reverse Diffusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66cd1ea",
      "metadata": {
        "id": "e66cd1ea"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t):\n",
        "    # Sample x_{t-1} given x_t using the model's noise prediction.\n",
        "    betas_t = get_index_from_list(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = get_index_from_list(\n",
        "        sqrt_recip_alphas, t, x.shape\n",
        "    )\n",
        "\n",
        "    # Equation 11 from DDPM.\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t / sqrt_one_minus_alphas_cumprod_t * model(x, t)\n",
        "    )\n",
        "\n",
        "    # If t == 0, we skip adding noise.\n",
        "    if (t == 0).all():\n",
        "        return model_mean\n",
        "\n",
        "    posterior_var_t = get_index_from_list(posterior_variance, t, x.shape)\n",
        "    noise = torch.randn_like(x)\n",
        "    return model_mean + torch.sqrt(posterior_var_t) * noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47eb142",
      "metadata": {
        "id": "f47eb142"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    # Iteratively sample from x_T ~ N(0, I) down to x_0.\n",
        "    b = shape[0]\n",
        "    img = torch.randn(shape, device=device)\n",
        "    for i in tqdm(reversed(range(T)), total=T, desc='Sampling', leave=False):\n",
        "        t = torch.full((b,), i, device=device, dtype=torch.long)\n",
        "        img = p_sample(model, img, t)\n",
        "    return img\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, n_samples=16):\n",
        "    model.eval()\n",
        "    imgs = p_sample_loop(\n",
        "        model,\n",
        "        shape=(n_samples, num_channels, image_size, image_size),\n",
        "    )\n",
        "    return imgs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcccb354",
      "metadata": {
        "id": "dcccb354"
      },
      "source": [
        "## 9. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4242bd38",
      "metadata": {
        "id": "4242bd38"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, num_epochs):\n",
        "    global_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for x, _ in pbar:\n",
        "            x = x.to(device)  # (B,1,28,28)\n",
        "            # Sample a timestep for each example\n",
        "            t = torch.randint(0, T, (x.shape[0],), device=device).long()\n",
        "\n",
        "            loss = p_losses(model, x, t)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}: avg loss = {avg_loss:.4f}')\n",
        "\n",
        "        # Generate sample images at the end of each epoch\n",
        "        with torch.no_grad():\n",
        "            samples = sample(model, n_samples=16)\n",
        "            show_image_grid(samples, title=f'Samples after epoch {epoch+1}')\n",
        "\n",
        "        # Optional: save checkpoint\n",
        "        torch.save(model.state_dict(), f'ddpm_mnist_epoch{epoch+1}.pth')\n",
        "\n",
        "# Uncomment this to start training in Colab.\n",
        "train(model, train_loader, num_epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73436440",
      "metadata": {
        "id": "73436440"
      },
      "source": [
        "## 10. Load a Trained Model and Generate Samples (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74eac00",
      "metadata": {
        "id": "c74eac00"
      },
      "outputs": [],
      "source": [
        "# Example usage if you already have a checkpoint:\n",
        "# model.load_state_dict(torch.load('ddpm_mnist_epoch10.pth', map_location=device))\n",
        "# model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     samples = sample(model, n_samples=16)\n",
        "#     show_image_grid(samples, title='Generated samples from trained DDPM (MNIST)')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}