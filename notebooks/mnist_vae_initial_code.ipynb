{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f533db",
      "metadata": {
        "id": "d8f533db"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50a6be7",
      "metadata": {
        "id": "c50a6be7"
      },
      "outputs": [],
      "source": [
        "# Configuration and hyperparameters\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# VAE hyperparameters\n",
        "vae_latent_dim = 16\n",
        "vae_hidden_dim = 400\n",
        "vae_epochs = 50\n",
        "vae_batch_size = 128\n",
        "vae_lr = 1e-3\n",
        "\n",
        "# Diffusion hyperparameters\n",
        "diffusion_T = 250         # number of diffusion steps\n",
        "diffusion_epochs = 50\n",
        "diffusion_batch_size = 128\n",
        "diffusion_lr = 2e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d12f7ba",
      "metadata": {
        "id": "5d12f7ba"
      },
      "outputs": [],
      "source": [
        "# MNIST dataset\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "train_loader_vae = DataLoader(train_dataset, batch_size=vae_batch_size, shuffle=True)\n",
        "test_loader_vae = DataLoader(test_dataset, batch_size=vae_batch_size, shuffle=False)\n",
        "\n",
        "print(\"Train size:\", len(train_dataset), \"Test size:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11c6d18",
      "metadata": {
        "id": "e11c6d18"
      },
      "source": [
        "## 1. VAE: MNIST → Latent → MNIST\n",
        "\n",
        "We use a simple fully-connected VAE:\n",
        "- Encoder: 784 → 400 → (μ, logσ²) in 16D latent space\n",
        "- Decoder: 16 → 400 → 784\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5651ee85",
      "metadata": {
        "id": "5651ee85"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=16):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc2(z))\n",
        "        logits = self.fc3(h)\n",
        "        x_hat = torch.sigmoid(logits)\n",
        "        return x_hat\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_flat = x.view(x.size(0), -1)\n",
        "        mu, logvar = self.encode(x_flat)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e362dee8",
      "metadata": {
        "id": "e362dee8"
      },
      "outputs": [],
      "source": [
        "def vae_loss(x, x_hat, mu, logvar):\n",
        "    x = x.view(x.size(0), -1)\n",
        "    bce = F.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return bce + kl, bce, kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5172a1a0",
      "metadata": {
        "id": "5172a1a0"
      },
      "outputs": [],
      "source": [
        "vae = VAE(input_dim=784, hidden_dim=vae_hidden_dim, latent_dim=vae_latent_dim).to(device)\n",
        "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=vae_lr)\n",
        "\n",
        "print(vae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d5e539",
      "metadata": {
        "id": "a7d5e539"
      },
      "outputs": [],
      "source": [
        "# Train VAE\n",
        "\n",
        "vae.train()\n",
        "for epoch in range(1, vae_epochs + 1):\n",
        "    total_loss = 0.0\n",
        "    total_bce = 0.0\n",
        "    total_kl = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for x, _ in train_loader_vae:\n",
        "        x = x.to(device)\n",
        "\n",
        "        vae_optimizer.zero_grad()\n",
        "        x_hat, mu, logvar = vae(x)\n",
        "        loss, bce, kl = vae_loss(x, x_hat, mu, logvar)\n",
        "        loss.backward()\n",
        "        vae_optimizer.step()\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        total_loss += loss.item()\n",
        "        total_bce += bce.item()\n",
        "        total_kl += kl.item()\n",
        "        n_samples += batch_size\n",
        "\n",
        "    avg_loss = total_loss / n_samples\n",
        "    avg_bce = total_bce / n_samples\n",
        "    avg_kl = total_kl / n_samples\n",
        "\n",
        "    print(f\"[VAE] Epoch {epoch:02d} | Loss: {avg_loss:.4f} (BCE {avg_bce:.4f}, KL {avg_kl:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c13189",
      "metadata": {
        "id": "f8c13189"
      },
      "outputs": [],
      "source": [
        "# Visualize some VAE reconstructions\n",
        "\n",
        "vae.eval()\n",
        "x_batch, y_batch = next(iter(test_loader_vae))\n",
        "x_batch = x_batch.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_hat, mu, logvar = vae(x_batch)\n",
        "\n",
        "x_batch = x_batch.cpu()\n",
        "x_hat = x_hat.cpu().view(-1, 1, 28, 28)\n",
        "\n",
        "n = 8\n",
        "plt.figure(figsize=(2 * n, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_batch[i, 0].numpy(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Orig: {y_batch[i].item()}\")\n",
        "\n",
        "    plt.subplot(2, n, n + i + 1)\n",
        "    plt.imshow(x_hat[i, 0].numpy(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Recon\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}