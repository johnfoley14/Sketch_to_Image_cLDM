{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6Fu2ahuSWWj"
      },
      "outputs": [],
      "source": [
        "# @title Setup: install & imports\n",
        "!pip install torch torchvision --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Tuple, List\n",
        "\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Random seed set to {seed}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "id": "A6Fu2ahuSWWj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibod1bdBSWWk"
      },
      "outputs": [],
      "source": [
        "# @title Experiment config\n",
        "USE_AUGMENTATION = True\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 0.1\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "PRINT_EVERY = 100        # batches\n",
        "NUM_WORKERS = 2          # increase if Colab runtime is beefy\n",
        "\n",
        "print('Config:')\n",
        "print(f' augmentation       : {USE_AUGMENTATION}')\n",
        "print(f' batch size         : {BATCH_SIZE}')\n",
        "print(f' epochs             : {EPOCHS}')\n",
        "print(f' lr / momentum / wd : {LEARNING_RATE} / {MOMENTUM} / {WEIGHT_DECAY}')\n",
        "print(f' device             : {device}')\n"
      ],
      "id": "Ibod1bdBSWWk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geT_Ru7DSWWl"
      },
      "outputs": [],
      "source": [
        "# @title Data pipeline (CIFAR-10)\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "if USE_AUGMENTATION:\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    ])\n",
        "else:\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=train_transform\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=test_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "\n",
        "classes = train_dataset.classes\n",
        "print('Classes:', classes)\n",
        "print('Train batches:', len(train_loader), 'Test batches:', len(test_loader))\n"
      ],
      "id": "geT_Ru7DSWWl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gedU-AweSWWm"
      },
      "outputs": [],
      "source": [
        "# @title Plain Block\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PlainBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Same internal conv/bn/relu structure but NO skip connection.\n",
        "    We still allow stride to downsample, but we don't add x.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out, inplace=True)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out = F.relu(out, inplace=True)\n",
        "        return out\n"
      ],
      "id": "gedU-AweSWWm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHu3qyabSWWm"
      },
      "outputs": [],
      "source": [
        "# @title Model definitions: PlainNet14CIFAR\n",
        "\n",
        "def make_layer(block_cls, in_channels, out_channels, num_blocks, first_stride):\n",
        "    \"\"\"\n",
        "    Helper to create a stage with `num_blocks` blocks.\n",
        "    first block can downsample via stride=2.\n",
        "    rest have stride=1.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    strides = [first_stride] + [1]*(num_blocks-1)\n",
        "    c_in = in_channels\n",
        "    for stride in strides:\n",
        "        layers.append(block_cls(c_in, out_channels, stride))\n",
        "        c_in = out_channels\n",
        "    return nn.Sequential(*layers), out_channels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PlainNet14CIFAR(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.layer1, c1 = make_layer(PlainBlock, 16, 16, num_blocks=2, first_stride=1)\n",
        "        self.layer2, c2 = make_layer(PlainBlock, c1, 32, num_blocks=2, first_stride=2)\n",
        "        self.layer3, c3 = make_layer(PlainBlock, c2, 64, num_blocks=2, first_stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc      = nn.Linear(c3, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out, inplace=True)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "plain14  = PlainNet14CIFAR().to(device)\n",
        "\n",
        "\n",
        "print('PlainNet14 params:', count_params(plain14))\n"
      ],
      "id": "LHu3qyabSWWm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGlD6JYVSWWn"
      },
      "outputs": [],
      "source": [
        "# @title Training / evaluation helpers\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, epoch_idx=0):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx + 1) % PRINT_EVERY == 0:\n",
        "            print(f\"[epoch {epoch_idx} | batch {batch_idx+1}/{len(loader)}] \"\n",
        "                  f\"loss={loss.item():.4f} \"\n",
        "                  f\"acc={(100.*correct/total):.2f}%\")\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    epoch_time = time.time() - start_time\n",
        "    return epoch_loss, epoch_acc, epoch_time\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for (inputs, targets) in loader:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ],
      "id": "hGlD6JYVSWWn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxGeUSxCSWWo"
      },
      "outputs": [],
      "source": [
        "# @title Train both models (ResNet14 vs PlainNet14)\n",
        "\n",
        "def run_experiment(model, train_loader, test_loader, epochs, lr, wd, momentum, label='model'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=lr, momentum=momentum, weight_decay=wd\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer,\n",
        "        milestones=[int(epochs*0.5)],\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc':  [],\n",
        "        'test_loss':  [],\n",
        "        'test_acc':   [],\n",
        "        'epoch_time': [],\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== Training {label} ===\")\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss, train_acc, train_time = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, epoch_idx=epoch\n",
        "        )\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "        history['epoch_time'].append(train_time)\n",
        "\n",
        "        print(f\"[{label}] Epoch {epoch}/{epochs} \"\n",
        "              f\"| train_acc={train_acc:.2f}% \"\n",
        "              f\"| test_acc={test_acc:.2f}% \"\n",
        "              f\"| train_loss={train_loss:.4f} \"\n",
        "              f\"| test_loss={test_loss:.4f} \"\n",
        "              f\"| time/epoch={train_time:.1f}s\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "plain14  = PlainNet14CIFAR()\n",
        "\n",
        "\n",
        "plain14, hist_plain = run_experiment(\n",
        "    plain14, train_loader, test_loader,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LEARNING_RATE,\n",
        "    wd=WEIGHT_DECAY,\n",
        "    momentum=MOMENTUM,\n",
        "    label='PlainNet14'\n",
        ")\n"
      ],
      "id": "wxGeUSxCSWWo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f3gt7STSWWp"
      },
      "outputs": [],
      "source": [
        "# @title Plot accuracy/loss comparison\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs_axis, hist_res['train_loss'], label='ResNet14 train')\n",
        "plt.plot(epochs_axis, hist_res['test_loss'],  label='ResNet14 test')\n",
        "plt.plot(epochs_axis, hist_plain['train_loss'], label='PlainNet14 train')\n",
        "plt.plot(epochs_axis, hist_plain['test_loss'],  label='PlainNet14 test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "id": "6f3gt7STSWWp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP1X2ZLaSWWp"
      },
      "outputs": [],
      "source": [
        "# @title Final comparison summary\n",
        "def summarize_hist(label, hist):\n",
        "    best_test_acc = max(hist['test_acc'])\n",
        "    last_test_acc = hist['test_acc'][-1]\n",
        "    avg_epoch_time = sum(hist['epoch_time']) / len(hist['epoch_time'])\n",
        "    print(f\"{label}:\")\n",
        "    print(f\"  best test acc : {best_test_acc:.2f}%\")\n",
        "    print(f\"  last test acc : {last_test_acc:.2f}%\")\n",
        "    print(f\"  avg epoch sec : {avg_epoch_time:.1f}s/epoch\")\n",
        "    print()\n",
        "\n",
        "\n",
        "summarize_hist('PlainNet14', hist_plain)\n",
        "\n",
        "print('Note:')\n",
        "print('- ResNet14 uses skip connections (residual learning).')\n",
        "print('- PlainNet14 is the same architecture but without the identity addition.')\n",
        "print('- Difference in accuracy shows how residual connections help optimization,')\n",
        "print('  especially as depth increases.')\n"
      ],
      "id": "LP1X2ZLaSWWp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flyamgPESWWq"
      },
      "outputs": [],
      "source": [
        "# @title Save checkpoints\n",
        "\n",
        "torch.save(plain14.state_dict(), 'plainnet14_cifar10.pth')\n"
      ],
      "id": "flyamgPESWWq"
    }
  ]
}