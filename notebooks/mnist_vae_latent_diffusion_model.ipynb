{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f533db",
      "metadata": {
        "id": "d8f533db"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50a6be7",
      "metadata": {
        "id": "c50a6be7"
      },
      "outputs": [],
      "source": [
        "# Configuration and hyperparameters\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# VAE hyperparameters\n",
        "vae_latent_dim = 16\n",
        "vae_hidden_dim = 400\n",
        "vae_epochs = 50\n",
        "vae_batch_size = 128\n",
        "vae_lr = 1e-3\n",
        "\n",
        "# Diffusion hyperparameters\n",
        "diffusion_T = 250         # number of diffusion steps\n",
        "diffusion_epochs = 50\n",
        "diffusion_batch_size = 128\n",
        "diffusion_lr = 2e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d12f7ba",
      "metadata": {
        "id": "5d12f7ba"
      },
      "outputs": [],
      "source": [
        "# MNIST dataset\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "train_loader_vae = DataLoader(train_dataset, batch_size=vae_batch_size, shuffle=True)\n",
        "test_loader_vae = DataLoader(test_dataset, batch_size=vae_batch_size, shuffle=False)\n",
        "\n",
        "print(\"Train size:\", len(train_dataset), \"Test size:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11c6d18",
      "metadata": {
        "id": "e11c6d18"
      },
      "source": [
        "## 1. VAE: MNIST → Latent → MNIST\n",
        "\n",
        "We use a simple fully-connected VAE:\n",
        "- Encoder: 784 → 400 → (μ, logσ²) in 16D latent space\n",
        "- Decoder: 16 → 400 → 784\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5651ee85",
      "metadata": {
        "id": "5651ee85"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=16):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc2(z))\n",
        "        logits = self.fc3(h)\n",
        "        x_hat = torch.sigmoid(logits)\n",
        "        return x_hat\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_flat = x.view(x.size(0), -1)\n",
        "        mu, logvar = self.encode(x_flat)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e362dee8",
      "metadata": {
        "id": "e362dee8"
      },
      "outputs": [],
      "source": [
        "def vae_loss(x, x_hat, mu, logvar):\n",
        "    x = x.view(x.size(0), -1)\n",
        "    bce = F.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return bce + kl, bce, kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5172a1a0",
      "metadata": {
        "id": "5172a1a0"
      },
      "outputs": [],
      "source": [
        "vae = VAE(input_dim=784, hidden_dim=vae_hidden_dim, latent_dim=vae_latent_dim).to(device)\n",
        "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=vae_lr)\n",
        "\n",
        "print(vae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d5e539",
      "metadata": {
        "id": "a7d5e539"
      },
      "outputs": [],
      "source": [
        "# Train VAE\n",
        "\n",
        "vae.train()\n",
        "for epoch in range(1, vae_epochs + 1):\n",
        "    total_loss = 0.0\n",
        "    total_bce = 0.0\n",
        "    total_kl = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for x, _ in train_loader_vae:\n",
        "        x = x.to(device)\n",
        "\n",
        "        vae_optimizer.zero_grad()\n",
        "        x_hat, mu, logvar = vae(x)\n",
        "        loss, bce, kl = vae_loss(x, x_hat, mu, logvar)\n",
        "        loss.backward()\n",
        "        vae_optimizer.step()\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        total_loss += loss.item()\n",
        "        total_bce += bce.item()\n",
        "        total_kl += kl.item()\n",
        "        n_samples += batch_size\n",
        "\n",
        "    avg_loss = total_loss / n_samples\n",
        "    avg_bce = total_bce / n_samples\n",
        "    avg_kl = total_kl / n_samples\n",
        "\n",
        "    print(f\"[VAE] Epoch {epoch:02d} | Loss: {avg_loss:.4f} (BCE {avg_bce:.4f}, KL {avg_kl:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c13189",
      "metadata": {
        "id": "f8c13189"
      },
      "outputs": [],
      "source": [
        "# Visualize some VAE reconstructions\n",
        "\n",
        "vae.eval()\n",
        "x_batch, y_batch = next(iter(test_loader_vae))\n",
        "x_batch = x_batch.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_hat, mu, logvar = vae(x_batch)\n",
        "\n",
        "x_batch = x_batch.cpu()\n",
        "x_hat = x_hat.cpu().view(-1, 1, 28, 28)\n",
        "\n",
        "n = 8\n",
        "plt.figure(figsize=(2 * n, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_batch[i, 0].numpy(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Orig: {y_batch[i].item()}\")\n",
        "\n",
        "    plt.subplot(2, n, n + i + 1)\n",
        "    plt.imshow(x_hat[i, 0].numpy(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Recon\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e93ef8b",
      "metadata": {
        "id": "4e93ef8b"
      },
      "source": [
        "## 2. Latent Diffusion: DDPM in VAE Latent Space\n",
        "\n",
        "We freeze the VAE and train a diffusion model on the latent vectors μ(x).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d87e860",
      "metadata": {
        "id": "2d87e860"
      },
      "outputs": [],
      "source": [
        "vae.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_to_latent(x):\n",
        "    x = x.to(device)\n",
        "    x_flat = x.view(x.size(0), -1)\n",
        "    mu, logvar = vae.encode(x_flat)\n",
        "    return mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d46b009",
      "metadata": {
        "id": "3d46b009"
      },
      "outputs": [],
      "source": [
        "# Diffusion utilities\n",
        "\n",
        "T = diffusion_T\n",
        "\n",
        "betas = torch.linspace(1e-4, 0.02, T)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "betas = betas.to(device)\n",
        "alphas = alphas.to(device)\n",
        "alphas_cumprod = alphas_cumprod.to(device)\n",
        "\n",
        "def q_sample(z0, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(z0)\n",
        "    sqrt_alpha_bar = torch.sqrt(alphas_cumprod[t]).view(-1, 1)\n",
        "    sqrt_one_minus = torch.sqrt(1.0 - alphas_cumprod[t]).view(-1, 1)\n",
        "    return sqrt_alpha_bar * z0 + sqrt_one_minus * noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3c26e7",
      "metadata": {
        "id": "9d3c26e7"
      },
      "outputs": [],
      "source": [
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, T, dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(T, dim)\n",
        "\n",
        "    def forward(self, t):\n",
        "        return self.emb(t)\n",
        "\n",
        "\n",
        "class LatentDenoiser(nn.Module):\n",
        "    def __init__(self, latent_dim, time_dim=32, hidden_dim=128, T=100):\n",
        "        super().__init__()\n",
        "        self.time_emb = TimeEmbedding(T, time_dim)\n",
        "        self.fc1 = nn.Linear(latent_dim + time_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, z_t, t):\n",
        "        t_emb = self.time_emb(t)\n",
        "        x = torch.cat([z_t, t_emb], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        eps_pred = self.fc3(x)\n",
        "        return eps_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f043bfd",
      "metadata": {
        "id": "9f043bfd"
      },
      "outputs": [],
      "source": [
        "diffusion_model = LatentDenoiser(\n",
        "    latent_dim=vae_latent_dim,\n",
        "    time_dim=32,\n",
        "    hidden_dim=128,\n",
        "    T=T\n",
        ").to(device)\n",
        "\n",
        "diffusion_optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=diffusion_lr)\n",
        "\n",
        "print(diffusion_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5528807",
      "metadata": {
        "id": "e5528807"
      },
      "outputs": [],
      "source": [
        "train_loader_diffusion = DataLoader(train_dataset, batch_size=diffusion_batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd11e0b6",
      "metadata": {
        "id": "cd11e0b6"
      },
      "outputs": [],
      "source": [
        "# Train diffusion model\n",
        "\n",
        "diffusion_model.train()\n",
        "\n",
        "for epoch in range(1, diffusion_epochs + 1):\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, _ in train_loader_diffusion:\n",
        "        x = x.to(device)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_flat = x.view(x.size(0), -1)\n",
        "            mu, logvar = vae.encode(x_flat)\n",
        "            z0 = mu\n",
        "\n",
        "        t = torch.randint(0, T, (batch_size,), device=device, dtype=torch.long)\n",
        "\n",
        "        noise = torch.randn_like(z0)\n",
        "        z_t = q_sample(z0, t, noise=noise)\n",
        "\n",
        "        eps_pred = diffusion_model(z_t, t)\n",
        "\n",
        "        loss = F.mse_loss(eps_pred, noise)\n",
        "\n",
        "        diffusion_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        diffusion_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / n_batches\n",
        "    print(f\"[Diffusion] Epoch {epoch:02d} | MSE loss: {avg_loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465150ad",
      "metadata": {
        "id": "465150ad"
      },
      "outputs": [],
      "source": [
        "# Sampling from diffusion + decode with VAE\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_step(z_t, t):\n",
        "    b = z_t.size(0)\n",
        "    t_tensor = torch.full((b,), t, device=device, dtype=torch.long)\n",
        "    eps_pred = diffusion_model(z_t, t_tensor)\n",
        "\n",
        "    beta_t = betas[t]\n",
        "    alpha_t = alphas[t]\n",
        "    alpha_bar_t = alphas_cumprod[t]\n",
        "    sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)\n",
        "\n",
        "    z0_est = (z_t - sqrt_one_minus_alpha_bar_t * eps_pred) / torch.sqrt(alpha_bar_t)\n",
        "\n",
        "    if t > 0:\n",
        "        alpha_bar_prev = alphas_cumprod[t - 1]\n",
        "        coef1 = torch.sqrt(alpha_bar_prev) * beta_t / (1.0 - alpha_bar_t)\n",
        "        coef2 = torch.sqrt(alpha_t) * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)\n",
        "        mean = coef1 * z0_est + coef2 * z_t\n",
        "        var = beta_t * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)\n",
        "        noise = torch.randn_like(z_t)\n",
        "        z_prev = mean + torch.sqrt(var) * noise\n",
        "    else:\n",
        "        z_prev = z0_est\n",
        "    return z_prev\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_latent_and_decode(n_samples=16):\n",
        "    diffusion_model.eval()\n",
        "    vae.eval()\n",
        "\n",
        "    z_t = torch.randn(n_samples, vae_latent_dim, device=device)\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        z_t = p_sample_step(z_t, t)\n",
        "\n",
        "    x_hat_flat = vae.decode(z_t)\n",
        "    x_hat = x_hat_flat.view(-1, 1, 28, 28)\n",
        "    return x_hat.cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626355ec",
      "metadata": {
        "id": "626355ec"
      },
      "outputs": [],
      "source": [
        "# Generate and visualize samples\n",
        "\n",
        "samples = sample_latent_and_decode(n_samples=16)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(samples[i, 0].numpy(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"VAE Latent Diffusion Samples\", y=0.92)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}