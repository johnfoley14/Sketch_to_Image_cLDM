{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "44f680f2",
      "metadata": {
        "id": "44f680f2"
      },
      "source": [
        "## 0. Environment check & imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b1fb7d1",
      "metadata": {
        "id": "0b1fb7d1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi || echo \"No GPU found\"\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d194cc71",
      "metadata": {
        "id": "d194cc71"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision tqdm pycocotools git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00035913",
      "metadata": {
        "id": "00035913"
      },
      "outputs": [],
      "source": [
        "import os, math, random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CocoCaptions\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e9bf54",
      "metadata": {
        "id": "45e9bf54"
      },
      "source": [
        "## 1. Download COCO val2017 images and captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531df08e",
      "metadata": {
        "id": "531df08e"
      },
      "outputs": [],
      "source": [
        "data_root = Path(\"./coco\")\n",
        "data_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# Download images (val2017)\n",
        "if not (data_root / \"val2017\").exists():\n",
        "    !wget -q http://images.cocodataset.org/zips/val2017.zip -O val2017.zip\n",
        "    !unzip -q val2017.zip -d coco\n",
        "\n",
        "# Download captions annotations\n",
        "if not (data_root / \"annotations\").exists():\n",
        "    !wget -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O annotations_trainval2017.zip\n",
        "    !unzip -q annotations_trainval2017.zip -d coco\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62cbb99c",
      "metadata": {
        "id": "62cbb99c"
      },
      "source": [
        "## 2. Dataset: images + raw captions (CLIP will tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91933b5b",
      "metadata": {
        "id": "91933b5b"
      },
      "outputs": [],
      "source": [
        "coco_root = \"./coco\"\n",
        "img_root = os.path.join(coco_root, \"val2017\")\n",
        "ann_file = os.path.join(coco_root, \"annotations\", \"captions_val2017.json\")\n",
        "\n",
        "raw_coco = CocoCaptions(root=img_root, annFile=ann_file)\n",
        "len(raw_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d75b427",
      "metadata": {
        "id": "0d75b427"
      },
      "outputs": [],
      "source": [
        "image_size = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),  # to [-1,1]\n",
        "])\n",
        "\n",
        "class CocoTextImageDataset(Dataset):\n",
        "    \"\"\"Returns (image_tensor, caption_string).\"\"\"\n",
        "    def __init__(self, root, ann_file, transform=None, subset_size=None):\n",
        "        self.coco = CocoCaptions(root=root, annFile=ann_file, transform=transform)\n",
        "        if subset_size is not None:\n",
        "            self.indices = list(range(min(subset_size, len(self.coco))))\n",
        "        else:\n",
        "            self.indices = list(range(len(self.coco)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        img, caps = self.coco[real_idx]\n",
        "        cap = random.choice(caps)\n",
        "        return img, cap\n",
        "\n",
        "subset_size = 4000  # adjust for Colab speed/quality trade-off\n",
        "dataset_full = CocoTextImageDataset(img_root, ann_file,\n",
        "                                    transform=transform,\n",
        "                                    subset_size=subset_size)\n",
        "\n",
        "# train/val split\n",
        "val_ratio = 0.1\n",
        "val_size = int(val_ratio * len(dataset_full))\n",
        "train_size = len(dataset_full) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset_full, [train_size, val_size])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                        shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "len(train_dataset), len(val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45700210",
      "metadata": {
        "id": "45700210"
      },
      "source": [
        "## 3. CLIP text encoder (conditioning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9981a5a0",
      "metadata": {
        "id": "9981a5a0"
      },
      "outputs": [],
      "source": [
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "\n",
        "clip_model.eval()\n",
        "for p in clip_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "with torch.no_grad():\n",
        "    dummy = clip.tokenize([\"hello world\"]).to(device)\n",
        "    dummy_emb = clip_model.encode_text(dummy)\n",
        "text_dim = dummy_emb.shape[-1]\n",
        "text_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ed534b",
      "metadata": {
        "id": "96ed534b"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def get_text_emb(captions, device=device):\n",
        "    tokens = clip.tokenize(captions, truncate=True).to(device)\n",
        "    text_features = clip_model.encode_text(tokens)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "    return text_features.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6854fded",
      "metadata": {
        "id": "6854fded"
      },
      "source": [
        "## 4. Convolutional VAE (image ↔ latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2421e075",
      "metadata": {
        "id": "2421e075"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch=3, latent_ch=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 32, 4, 2, 1),  # 64 -> 32\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),     # 32 -> 16\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),    # 16 -> 16\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.mu_conv = nn.Conv2d(128, latent_ch, 1)\n",
        "        self.logvar_conv = nn.Conv2d(128, latent_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        mu = self.mu_conv(h)\n",
        "        logvar = self.logvar_conv(h)\n",
        "        return mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, out_ch=3, latent_ch=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(latent_ch, 128, 3, 1, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 16 -> 32\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),   # 32 -> 64\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, out_ch, 3, 1, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, latent_ch=4):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, latent_ch)\n",
        "        self.decoder = Decoder(in_ch, latent_ch)\n",
        "\n",
        "    def encode(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, logvar = self.encode(x)\n",
        "        x_rec = self.decode(z)\n",
        "        return x_rec, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b3fa1d",
      "metadata": {
        "id": "c9b3fa1d"
      },
      "outputs": [],
      "source": [
        "vae_latent_ch = 4\n",
        "vae = VAE(in_ch=3, latent_ch=vae_latent_ch).to(device)\n",
        "sum(p.numel() for p in vae.parameters()) / 1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79228f3",
      "metadata": {
        "id": "e79228f3"
      },
      "outputs": [],
      "source": [
        "def vae_loss(x, x_rec, mu, logvar, beta_kl=1e-3):\n",
        "    recon = F.mse_loss(x_rec, x, reduction='mean')\n",
        "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon + beta_kl * kl, recon, kl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5eaddf2",
      "metadata": {
        "id": "c5eaddf2"
      },
      "source": [
        "### 4.1 Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413ffdb1",
      "metadata": {
        "id": "413ffdb1"
      },
      "outputs": [],
      "source": [
        "vae_epochs = 20\n",
        "vae_lr = 1e-3\n",
        "opt_vae = torch.optim.Adam(vae.parameters(), lr=vae_lr)\n",
        "\n",
        "for epoch in range(1, vae_epochs+1):\n",
        "    vae.train()\n",
        "    tot, rec, kl_sum, n = 0.0, 0.0, 0.0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"VAE Epoch {epoch}/{vae_epochs}\")\n",
        "    for imgs, caps in pbar:\n",
        "        imgs = imgs.to(device)\n",
        "        x_rec, mu, logvar = vae(imgs)\n",
        "        loss, recon, kl = vae_loss(imgs, x_rec, mu, logvar)\n",
        "\n",
        "        opt_vae.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_vae.step()\n",
        "\n",
        "        tot += loss.item(); rec += recon.item(); kl_sum += kl.item(); n += 1\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"recon\": f\"{recon.item():.4f}\"})\n",
        "\n",
        "    print(f\"[VAE] Epoch {epoch}: total={tot/n:.4f}, recon={rec/n:.4f}, kl={kl_sum/n:.4f}\")\n",
        "\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        imgs, caps = next(iter(val_loader))\n",
        "        imgs = imgs.to(device)\n",
        "        x_rec, mu, logvar = vae(imgs)\n",
        "        grid = make_grid(torch.cat([imgs, x_rec], dim=0), nrow=imgs.size(0))\n",
        "        os.makedirs(\"vae_recon\", exist_ok=True)\n",
        "        save_image((grid+1)/2, f\"vae_recon/epoch_{epoch:03d}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualize VAE reconstructions from the validation set ---\n",
        "\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    # get one batch from validation loader\n",
        "    imgs, caps = next(iter(val_loader))\n",
        "    imgs = imgs.to(device)\n",
        "\n",
        "    # pass through VAE\n",
        "    x_rec, mu, logvar = vae(imgs)\n",
        "\n",
        "# make a grid: first row originals, second row reconstructions\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "grid = make_grid(\n",
        "    torch.cat([imgs, x_rec], dim=0),  # stack originals + recon\n",
        "    nrow=imgs.size(0)                 # one row per set (orig row, recon row)\n",
        ")\n",
        "\n",
        "# images are in [-1, 1] → convert to [0, 1] for display\n",
        "grid_np = (grid.permute(1, 2, 0).cpu().numpy() + 1) / 2.0\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(grid_np)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Top: original images  |  Bottom: VAE reconstructions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wh2eUiPU9WCW"
      },
      "id": "Wh2eUiPU9WCW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5626921d",
      "metadata": {
        "id": "5626921d"
      },
      "source": [
        "## 5. Diffusion process in latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1c6e6b",
      "metadata": {
        "id": "8f1c6e6b"
      },
      "outputs": [],
      "source": [
        "def make_beta_schedule(T, beta_start=1e-4, beta_end=0.02):\n",
        "    return torch.linspace(beta_start, beta_end, T)\n",
        "\n",
        "T = 200\n",
        "betas = make_beta_schedule(T).to(device)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "alphas_cumprod_prev = torch.cat([torch.tensor([1.], device=device), alphas_cumprod[:-1]], dim=0)\n",
        "\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38977be6",
      "metadata": {
        "id": "38977be6"
      },
      "outputs": [],
      "source": [
        "def q_sample_latent(z0, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(z0)\n",
        "    sqrt_ac = sqrt_alphas_cumprod[t].view(-1,1,1,1)\n",
        "    sqrt_om = sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n",
        "    return sqrt_ac * z0 + sqrt_om * noise\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, t):\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
        "        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "        if self.dim % 2 == 1:\n",
        "            emb = F.pad(emb, (0, 1))\n",
        "        return emb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb2e6532",
      "metadata": {
        "id": "cb2e6532"
      },
      "source": [
        "## 6. UNet with cross-attention in latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75c3001",
      "metadata": {
        "id": "a75c3001"
      },
      "outputs": [],
      "source": [
        "class CrossAttention2D(nn.Module):\n",
        "    def __init__(self, channels, ctx_dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (channels // num_heads) ** -0.5\n",
        "        self.q = nn.Linear(channels, channels)\n",
        "        self.k = nn.Linear(ctx_dim, channels)\n",
        "        self.v = nn.Linear(ctx_dim, channels)\n",
        "        self.proj = nn.Linear(channels, channels)\n",
        "\n",
        "    def forward(self, x, ctx):\n",
        "        B, C, H, W = x.shape\n",
        "        x_flat = x.permute(0, 2, 3, 1).view(B, H*W, C)\n",
        "        ctx = ctx.unsqueeze(1)\n",
        "\n",
        "        q = self.q(x_flat)\n",
        "        k = self.k(ctx)\n",
        "        v = self.v(ctx)\n",
        "\n",
        "        def split_heads(t):\n",
        "            B, L, D = t.shape\n",
        "            head_dim = D // self.num_heads\n",
        "            t = t.view(B, L, self.num_heads, head_dim).transpose(1, 2)\n",
        "            return t\n",
        "\n",
        "        q = split_heads(q)\n",
        "        k = split_heads(k)\n",
        "        v = split_heads(v)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-2)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, H*W, C)\n",
        "        out = self.proj(out)\n",
        "        out = out.view(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        return x + out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04fb5d0",
      "metadata": {
        "id": "f04fb5d0"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(8, in_ch)\n",
        "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
        "        self.act = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        self.time_fc = nn.Linear(emb_dim, out_ch)\n",
        "        self.text_fc = nn.Linear(emb_dim, out_ch)\n",
        "\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb, txt_emb):\n",
        "        h = self.norm1(x)\n",
        "        h = self.act(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        cond = self.time_fc(t_emb) + self.text_fc(txt_emb)\n",
        "        h = h + cond[:, :, None, None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = self.act(h)\n",
        "        h = self.conv2(h)\n",
        "        return h + self.skip(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8510e6a6",
      "metadata": {
        "id": "8510e6a6"
      },
      "outputs": [],
      "source": [
        "class LatentUNet(nn.Module):\n",
        "    def __init__(self, latent_ch=4, base_ch=64, emb_dim=512, ctx_dim=512):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            TimeEmbedding(emb_dim),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "        self.conv_in = nn.Conv2d(latent_ch, base_ch, 3, padding=1)\n",
        "        self.down1 = ResBlock(base_ch, base_ch, emb_dim)\n",
        "        self.down2 = ResBlock(base_ch, base_ch*2, emb_dim)\n",
        "        self.down3 = ResBlock(base_ch*2, base_ch*4, emb_dim)\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(2)\n",
        "\n",
        "        self.mid1 = ResBlock(base_ch*4, base_ch*4, emb_dim)\n",
        "        self.mid_attn = CrossAttention2D(base_ch*4, ctx_dim)\n",
        "        self.mid2 = ResBlock(base_ch*4, base_ch*4, emb_dim)\n",
        "\n",
        "        self.up3 = ResBlock(base_ch*4, base_ch*2, emb_dim)\n",
        "        self.up2 = ResBlock(base_ch*2, base_ch, emb_dim)\n",
        "        self.up1 = ResBlock(base_ch, base_ch, emb_dim)\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
        "        self.conv_out = nn.Conv2d(base_ch, latent_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, zt, t, txt_emb):\n",
        "        t_emb = self.time_mlp(t)\n",
        "\n",
        "        x0 = self.conv_in(zt)\n",
        "        x1 = self.down1(x0, t_emb, txt_emb)\n",
        "        x2 = self.downsample(x1)\n",
        "        x2 = self.down2(x2, t_emb, txt_emb)\n",
        "        x3 = self.downsample(x2)\n",
        "        x3 = self.down3(x3, t_emb, txt_emb)\n",
        "        x4 = self.downsample(x3)\n",
        "\n",
        "        m = self.mid1(x4, t_emb, txt_emb)\n",
        "        m = self.mid_attn(m, txt_emb)\n",
        "        m = self.mid2(m, t_emb, txt_emb)\n",
        "\n",
        "        u3 = self.upsample(m) + x3\n",
        "        u3 = self.up3(u3, t_emb, txt_emb)\n",
        "        u2 = self.upsample(u3) + x2\n",
        "        u2 = self.up2(u2, t_emb, txt_emb)\n",
        "        u1 = self.upsample(u2) + x1\n",
        "        u1 = self.up1(u1, t_emb, txt_emb)\n",
        "\n",
        "        return self.conv_out(u1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d910644d",
      "metadata": {
        "id": "d910644d"
      },
      "outputs": [],
      "source": [
        "base_ch = 64\n",
        "diff_model = LatentUNet(latent_ch=vae_latent_ch,\n",
        "                        base_ch=base_ch,\n",
        "                        emb_dim=text_dim,\n",
        "                        ctx_dim=text_dim).to(device)\n",
        "sum(p.numel() for p in diff_model.parameters()) / 1e6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25762dd4",
      "metadata": {
        "id": "25762dd4"
      },
      "source": [
        "## 7. Diffusion training in latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f52858",
      "metadata": {
        "id": "59f52858"
      },
      "outputs": [],
      "source": [
        "def get_t(batch_size, T, device):\n",
        "    return torch.randint(0, T, (batch_size,), device=device)\n",
        "\n",
        "def p_losses_latent(model, vae, x, captions, t):\n",
        "    with torch.no_grad():\n",
        "        z0, mu, logvar = vae.encode(x)\n",
        "    noise = torch.randn_like(z0)\n",
        "    zt = q_sample_latent(z0, t, noise)\n",
        "    txt_emb = get_text_emb(captions, device=device)\n",
        "    pred_noise = model(zt, t, txt_emb)\n",
        "    return F.mse_loss(pred_noise, noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0583ced2",
      "metadata": {
        "id": "0583ced2"
      },
      "outputs": [],
      "source": [
        "diff_epochs = 20\n",
        "lr = 2e-4\n",
        "opt_diff = torch.optim.Adam(diff_model.parameters(), lr=lr)\n",
        "\n",
        "for p in vae.parameters():\n",
        "    p.requires_grad = False\n",
        "vae.eval()\n",
        "\n",
        "fixed_prompts = [\n",
        "    \"a man riding a bicycle on a street\",\n",
        "    \"a dog running on the grass\",\n",
        "    \"a group of people sitting at a table\",\n",
        "    \"a plate of food on a table\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c916f0ba",
      "metadata": {
        "id": "c916f0ba"
      },
      "source": [
        "### 7.1 Reverse diffusion step and sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700d47c7",
      "metadata": {
        "id": "700d47c7"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample_latent_step(model, zt, t, txt_emb):\n",
        "    pred_noise = model(zt, t, txt_emb)\n",
        "\n",
        "    beta_t = betas[t].view(-1,1,1,1)\n",
        "    alpha_t = alphas[t].view(-1,1,1,1)\n",
        "    ac      = alphas_cumprod[t].view(-1,1,1,1)\n",
        "    ac_prev = alphas_cumprod_prev[t].view(-1,1,1,1)\n",
        "\n",
        "    sqrt_one_minus_ac = sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n",
        "    sqrt_recip_ac = torch.sqrt(1.0 / ac)\n",
        "    z0_hat = (zt - sqrt_one_minus_ac * pred_noise) * sqrt_recip_ac\n",
        "\n",
        "    posterior_mean = (\n",
        "        torch.sqrt(ac_prev) * beta_t / (1.0 - ac) * z0_hat +\n",
        "        torch.sqrt(alpha_t) * (1.0 - ac_prev) / (1.0 - ac) * zt\n",
        "    )\n",
        "\n",
        "    noise = torch.randn_like(zt)\n",
        "    var = posterior_variance[t].view(-1,1,1,1)\n",
        "    nonzero_mask = (t > 0).float().view(-1,1,1,1)\n",
        "    return posterior_mean + nonzero_mask * torch.sqrt(var) * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821f8414",
      "metadata": {
        "id": "821f8414"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_latent_and_decode(model, vae, prompts, n_steps=T):\n",
        "    model_was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    b = len(prompts)\n",
        "    txt_emb = get_text_emb(prompts, device=device)\n",
        "    zt = torch.randn(b, vae_latent_ch, image_size//4, image_size//4, device=device)\n",
        "\n",
        "    for i in reversed(range(n_steps)):\n",
        "        t = torch.full((b,), i, device=device, dtype=torch.long)\n",
        "        zt = p_sample_latent_step(model, zt, t, txt_emb)\n",
        "\n",
        "    z0 = zt.clamp(-5, 5)\n",
        "    x0 = vae.decode(z0).clamp(-1, 1)\n",
        "\n",
        "    if model_was_training:\n",
        "        model.train()\n",
        "    return x0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c97c7b",
      "metadata": {
        "id": "c7c97c7b"
      },
      "outputs": [],
      "source": [
        "global_step = 0\n",
        "sample_every = 400\n",
        "\n",
        "for epoch in range(1, diff_epochs+1):\n",
        "    diff_model.train()\n",
        "    epoch_losses = []\n",
        "    pbar = tqdm(train_loader, desc=f\"Diffusion Epoch {epoch}/{diff_epochs}\")\n",
        "    for imgs, caps in pbar:\n",
        "        imgs = imgs.to(device)\n",
        "        B = imgs.size(0)\n",
        "        t = get_t(B, T, device)\n",
        "\n",
        "        loss = p_losses_latent(diff_model, vae, imgs, caps, t)\n",
        "        opt_diff.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_diff.step()\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "        global_step += 1\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        if global_step % sample_every == 0:\n",
        "            with torch.no_grad():\n",
        "                samples = sample_latent_and_decode(diff_model, vae, fixed_prompts, n_steps=T)\n",
        "                grid = make_grid((samples+1)/2, nrow=2)\n",
        "                os.makedirs(\"samples_latent_clip\", exist_ok=True)\n",
        "                save_image(grid, f\"samples_latent_clip/step_{global_step:06d}.png\")\n",
        "\n",
        "    print(f\"[Diffusion] Epoch {epoch}: train_loss={sum(epoch_losses)/len(epoch_losses):.4f}\")\n",
        "\n",
        "    diff_model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, caps in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            t = get_t(B, T, device)\n",
        "            loss = p_losses_latent(diff_model, vae, imgs, caps, t)\n",
        "            val_losses.append(loss.item())\n",
        "    print(f\"[Diffusion] Epoch {epoch}: val_loss={sum(val_losses)/len(val_losses):.4f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        samples = sample_latent_and_decode(diff_model, vae, fixed_prompts, n_steps=T)\n",
        "        grid = make_grid((samples+1)/2, nrow=2)\n",
        "        os.makedirs(\"samples_latent_clip\", exist_ok=True)\n",
        "        save_image(grid, f\"samples_latent_clip/epoch_{epoch:03d}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f625580",
      "metadata": {
        "id": "0f625580"
      },
      "source": [
        "## 8. Inference: generate from custom prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2584b1",
      "metadata": {
        "id": "bc2584b1"
      },
      "outputs": [],
      "source": [
        "user_prompts = [\n",
        "    \"a cat sitting on a chair\",\n",
        "    \"a person skiing on a snowy mountain\",\n",
        "    \"a red car driving on a road\",\n",
        "    \"a cup of coffee on a table\",\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    samples = sample_latent_and_decode(diff_model, vae, user_prompts, n_steps=T)\n",
        "    grid = make_grid((samples+1)/2, nrow=2)\n",
        "    os.makedirs(\"samples_latent_clip\", exist_ok=True)\n",
        "    save_image(grid, \"samples_latent_clip/user_prompts.png\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(grid.permute(1,2,0).cpu().numpy())\n",
        "plt.axis(\"off\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}